{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBHyofeV98"
      },
      "source": [
        "# Lab 02: Basic NLP Preprocessing Techniques\n",
        "\n",
        "**Course:** ITAI 2373 - Natural Language Processing  \n",
        "**Module:** 02 - Text Preprocessing  \n",
        "**Duration:** 2-3 hours  \n",
        "**Student Name:** Chloe Tu\n",
        "\n",
        "**Date:** 9/8/2025\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By completing this lab, you will:\n",
        "1. Understand the critical role of preprocessing in NLP pipelines\n",
        "2. Master fundamental text preprocessing techniques\n",
        "3. Compare different libraries and their approaches\n",
        "4. Analyze the effects of preprocessing on text data\n",
        "5. Build a complete preprocessing pipeline\n",
        "6. Load and work with different types of text datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-L6J3seV99"
      },
      "source": [
        "## üìñ Introduction to NLP Preprocessing\n",
        "\n",
        "Natural Language Processing (NLP) preprocessing refers to the initial steps taken to clean and transform raw text data into a format that's more suitable for analysis by machine learning algorithms.\n",
        "\n",
        "### Why is preprocessing crucial?\n",
        "\n",
        "1. **Standardization:** Ensures consistent text format across your dataset\n",
        "2. **Noise Reduction:** Removes irrelevant information that could confuse algorithms\n",
        "3. **Complexity Reduction:** Simplifies text to focus on meaningful patterns\n",
        "4. **Performance Enhancement:** Improves the efficiency and accuracy of downstream tasks\n",
        "\n",
        "### Real-world Impact\n",
        "Consider searching for \"running shoes\" vs \"Running Shoes!\" - without preprocessing, these might be treated as completely different queries. Preprocessing ensures they're recognized as equivalent."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§î Conceptual Question 1\n",
        "**Before we start coding, think about your daily interactions with text processing systems (search engines, chatbots, translation apps). What challenges do you think these systems face when processing human language? List at least 3 specific challenges and explain why each is problematic.**\n",
        "\n",
        "Here are three big challenges text systems face from my perspective:\n",
        "\n",
        "**Challenge 1: Words have many meanings.** Think about the word \"bank.\" It could be a place where you keep money, or the ground beside a river. People understand which \"bank\" is meant by looking at the rest of the sentence or conversation. But for a computer, it's hard to know the right meaning without really understanding the context, which can lead to mistakes in search results or chatbot answers.\n",
        "\n",
        "**Challenge 2: People say the same thing in different ways.** I see people use synonyms, like \"car\" and \"automobile,\" or phrase things differently, like \"buy a ticket\" versus \"purchase a ticket.\" A computer needs to be smart enough to recognize that these different wordings mean the same thing. If it doesn't, a search engine might miss relevant results, or a translation app might not capture the full sense of what someone is trying to say.\n",
        "\n",
        "**Challenge 3: Language is messy and changes.** People use slang, abbreviations (like \"LOL\"), make typos, and even use emojis to express feelings. Plus, new words and phrases pop up all the time. Computers are usually trained on clean, formal text. This makes it tough for them to understand everyday language, social media posts, or things said informally. They have to constantly learn and adapt to keep up with how people actually talk and write.\n",
        "\n",
        "* * *"
      ],
      "metadata": {
        "id": "8uFv0IOTweoT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXDC1YX1eV99"
      },
      "source": [
        "## üõ†Ô∏è Part 1: Environment Setup\n",
        "\n",
        "We'll be working with two major NLP libraries:\n",
        "- **NLTK (Natural Language Toolkit):** Comprehensive NLP library with extensive resources\n",
        "- **spaCy:** Industrial-strength NLP with pre-trained models\n",
        "\n",
        "**‚ö†Ô∏è Note:** Installation might take 2-3 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IjSAdym1eV99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150e6348-a213-4c13-d33f-965d5209594f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Installing NLP libraries...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "print(\"üîß Installing NLP libraries...\")\n",
        "\n",
        "!pip install -q nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24f9b301"
      },
      "source": [
        "### ü§î Conceptual Question 2\n",
        "**Why do you think we need to install a separate language model (en_core_web_sm) for spaCy? What components might this model contain that help with text processing? Think about what information a computer needs to understand English text.**\n",
        "\n",
        "\n",
        "\n",
        "I think I need to install a language model like `en_core_web_sm` for spaCy because spaCy isn't just about splitting text into words; it needs to understand the English language to do more complex tasks. Think of the model as spaCy's brain for English. This brain contains a huge vocabulary, words that understand the relationships between words helping it grasp meaning, syntax rules that know how English sentences are put together to figure out sentence structure, and is trained to spot named entities like people, places, organizations, and dates, which is super helpful for extracting information. Basically, the model gives spaCy the background knowledge and patterns it needs to process English text intelligently, not just as a string of characters.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uqponRiceV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb445cd-74a7-4029-f361-51aa830f607c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Downloading NLTK data packages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ All imports and downloads completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Import Libraries and Download NLTK Data\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Download essential NLTK data\n",
        "print(\"üì¶ Downloading NLTK data packages...\")\n",
        "nltk.download('punkt')      # For tokenization\n",
        "nltk.download('stopwords')  # For stop word removal\n",
        "nltk.download('wordnet')    # For lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "nltk.download('punkt_tab') # Download punkt_tab resource\n",
        "\n",
        "print(\"\\n‚úÖ All imports and downloads completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQye-Ye2eV9-"
      },
      "source": [
        "## üìÇ Part 2: Sample Text Data\n",
        "\n",
        "We'll work with different types of text to understand how preprocessing affects various text styles:\n",
        "- Simple text\n",
        "- Academic text (with citations, URLs)\n",
        "- Social media text (with emojis, hashtags)\n",
        "- News text (formal writing)\n",
        "- Product reviews (informal, ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QNF6oBpFeV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef85713-9eeb-4664-ef38-8b00d8e57ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Sample texts loaded successfully!\n",
            "\n",
            "üè∑Ô∏è Simple: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "üè∑Ô∏è Academic: Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
            "She publi...\n",
            "\n",
            "üè∑Ô∏è Social Media: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yu...\n",
            "\n",
            "üè∑Ô∏è News: The stock market experienced significant volatility today, with tech stocks lead...\n",
            "\n",
            "üè∑Ô∏è Product Review: This laptop is absolutely fantastic! I've been using it for 6 months and it's st...\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Load Sample Texts\n",
        "simple_text = \"Natural Language Processing is a fascinating field of AI. It's amazing!\"\n",
        "\n",
        "academic_text = \"\"\"\n",
        "Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
        "She published 3 papers in 2023, focusing on deep neural networks (DNNs).\n",
        "The results were amazing - accuracy improved by 15.7%!\n",
        "\"This is revolutionary,\" said Prof. Johnson.\n",
        "Visit https://example.com for more info. #NLP #AI @university\n",
        "\"\"\"\n",
        "\n",
        "social_text = \"OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\"\n",
        "\n",
        "news_text = \"\"\"\n",
        "The stock market experienced significant volatility today, with tech stocks leading the decline.\n",
        "Apple Inc. (AAPL) dropped 3.2%, while Microsoft Corp. fell 2.8%.\n",
        "\"We're seeing a rotation out of growth stocks,\" said analyst Jane Doe from XYZ Capital.\n",
        "\"\"\"\n",
        "\n",
        "review_text = \"\"\"\n",
        "This laptop is absolutely fantastic! I've been using it for 6 months and it's still super fast.\n",
        "The battery life is incredible - lasts 8-10 hours easily.\n",
        "Only complaint: the keyboard could be better. Overall rating: 4.5/5 stars.\n",
        "\"\"\"\n",
        "\n",
        "# Store all texts\n",
        "sample_texts = {\n",
        "    \"Simple\": simple_text,\n",
        "    \"Academic\": academic_text.strip(),\n",
        "    \"Social Media\": social_text,\n",
        "    \"News\": news_text.strip(),\n",
        "    \"Product Review\": review_text.strip()\n",
        "}\n",
        "\n",
        "print(\"üìÑ Sample texts loaded successfully!\")\n",
        "for name, text in sample_texts.items():\n",
        "    preview = text[:80] + \"...\" if len(text) > 80 else text\n",
        "    print(f\"\\nüè∑Ô∏è {name}: {preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6216f3db"
      },
      "source": [
        "### ü§î Conceptual Question 3\n",
        "**Looking at the different text types we've loaded, what preprocessing challenges do you anticipate for each type? For each text type below, identify at least 2 specific preprocessing challenges and explain why they might be problematic for NLP analysis.**\n",
        "\n",
        "\n",
        "**Simple text challenges:**\n",
        "1. Even in simple text, ambiguity can pop up. For example, \"AI is amazing!\" could mean \"Artificial Intelligence\" or refer to a person named \"AI.\" It's usually clear to people from context, but for a machine, it might struggle to pick the right meaning without more information.\n",
        "2. Contractions like \"It's\" can be a challenge. Do you keep them as one token or split them into \"It\" and \"is\"? The choice can affect things like vocabulary size and how relationships between words are learned.\n",
        "\n",
        "**Academic text challenges:**\n",
        "1. Academic text often uses citations (like \"(Smith, 2023)\") and references URLs. These aren't usually important for understanding the core content and just add noise. Removing or handling them correctly is needed so they don't interfere with analysis.\n",
        "2. It uses lots of technical terms and abbreviations (like \"DNNs\"). NLP models might not recognize these, or they could have different meanings in other fields. Understanding or standardizing this specialized language is important.\n",
        "\n",
        "**Social media text challenges:**\n",
        "1. This text is full of informal language, slang, and abbreviations (\"OMG,\" \"SO GOOD!!!\"). It also includes things like emojis and hashtags. This \"messiness\" makes it hard for traditional NLP methods that expect cleaner text.\n",
        "2. Mentions (@username) and URLs are common. These often don't add semantic value to the message itself and can be removed to simplify the text, but you might lose information about who was mentioned or linked.\n",
        "\n",
        "**News text challenges:**\n",
        "1. News articles often contain named entities like company names (Apple Inc., Microsoft Corp.) and people's names (Jane Doe). Identifying these correctly is important for information extraction, but capitalization or variations in naming can make it tricky.\n",
        "2. Quotes from people (\"We're seeing...\") include contractions and sometimes slightly less formal language than the main body of the text. Handling these nested structures and potentially different language styles within the same text can be complex.\n",
        "\n",
        "**Product review challenges:**\n",
        "1. Reviews often use informal language, misspellings, and emotional words (\"absolutely fantastic,\" \"super fast\"). Capturing the sentiment accurately requires handling these variations and understanding how words contribute to positive or negative feelings.\n",
        "2. Ratings (like \"4.5/5 stars\") are structured data within the text. Extracting this numerical information and linking it to the textual review requires specific parsing techniques beyond standard text processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLLnDF9YeV9-"
      },
      "source": [
        "## üî§ Part 3: Tokenization\n",
        "\n",
        "### What is Tokenization?\n",
        "Tokenization is the process of breaking down text into smaller, meaningful units called **tokens**. These tokens are typically words, but can also be sentences, characters, or subwords.\n",
        "\n",
        "### Why is it Important?\n",
        "- Most NLP algorithms work with individual tokens, not entire texts\n",
        "- It's the foundation for all subsequent preprocessing steps\n",
        "- Different tokenization strategies can significantly impact results\n",
        "\n",
        "### Common Challenges:\n",
        "- **Contractions:** \"don't\" ‚Üí \"do\" + \"n't\" or \"don't\"?\n",
        "- **Punctuation:** Keep with words or separate?\n",
        "- **Special characters:** How to handle @, #, URLs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s3T8WpUheV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4c63e4-f05b-4f83-d594-7fc65ab9a3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç NLTK Tokenization Results\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Word tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "Number of tokens: 14\n",
            "\n",
            "Sentences: ['Natural Language Processing is a fascinating field of AI.', \"It's amazing!\"]\n",
            "Number of sentences: 2\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Tokenization with NLTK\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Test on simple text\n",
        "print(\"üîç NLTK Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Word tokenization\n",
        "nltk_tokens = word_tokenize(simple_text)\n",
        "print(f\"\\nWord tokens: {nltk_tokens}\")\n",
        "print(f\"Number of tokens: {len(nltk_tokens)}\")\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(simple_text)\n",
        "print(f\"\\nSentences: {sentences}\")\n",
        "print(f\"Number of sentences: {len(sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1eb01f"
      },
      "source": [
        "### ü§î Conceptual Question 4\n",
        "**Examine the NLTK tokenization results above. How did NLTK handle the contraction \"It's\"? What happened to the punctuation marks? Do you think this approach is appropriate for all NLP tasks? Explain your reasoning.**\n",
        "\n",
        "\n",
        "**How \"It's\" was handled:** NLTK split the contraction \"It's\" into two separate tokens: \"It\" and \"'s\". This approach separates the pronoun from the contracted form of \"is\" or \"has\".\n",
        "\n",
        "**Punctuation treatment:** NLTK treated punctuation marks like the period \".\" and exclamation mark \"!\" as individual tokens, distinct from the words they are attached to.\n",
        "\n",
        "**Appropriateness for different tasks:** This method of splitting contractions and separating punctuation is a common approach in tokenization and is appropriate for many NLP tasks where the focus is on individual words or the strict separation of tokens. However, for some tasks, like sentiment analysis or preserving the exact original text structure for linguistic analysis, keeping contractions as single tokens or handling punctuation differently might be necessary. For instance, in sentiment analysis, \"It's amazing!\" has a clear positive sentiment, and splitting \"It's\" doesn't usually hurt, but consider \"It's not good\" where splitting \"not\" is crucial, which NLTK does. The handling of punctuation as separate tokens is generally beneficial as punctuation can indicate sentence boundaries or grammatical structure, but in cases like emoticons (e.g., \":)\") or specific domain text, the punctuation might be an integral part of a meaningful token. Therefore, while NLTK's approach is a solid default, its appropriateness depends on the specific goals and requirements of the NLP task.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TQKNF-yceV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30a4beb-ca59-4af7-f932-dbc300057e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç spaCy Tokenization Results\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Word tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "Number of tokens: 14\n",
            "\n",
            "üî¨ Detailed Token Analysis:\n",
            "Token        POS      Lemma        Is Alpha Is Stop \n",
            "--------------------------------------------------\n",
            "Natural      PROPN    Natural      1        0       \n",
            "Language     PROPN    Language     1        0       \n",
            "Processing   NOUN     processing   1        0       \n",
            "is           AUX      be           1        1       \n",
            "a            DET      a            1        1       \n",
            "fascinating  ADJ      fascinating  1        0       \n",
            "field        NOUN     field        1        0       \n",
            "of           ADP      of           1        1       \n",
            "AI           PROPN    AI           1        0       \n",
            ".            PUNCT    .            0        0       \n",
            "It           PRON     it           1        1       \n",
            "'s           AUX      be           0        1       \n",
            "amazing      ADJ      amazing      1        0       \n",
            "!            PUNCT    !            0        0       \n"
          ]
        }
      ],
      "source": [
        "# Step 5: Tokenization with spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"üîç spaCy Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Process with spaCy\n",
        "doc = nlp(simple_text)\n",
        "\n",
        "# Extract tokens\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(f\"\\nWord tokens: {spacy_tokens}\")\n",
        "print(f\"Number of tokens: {len(spacy_tokens)}\")\n",
        "\n",
        "# Show detailed token information\n",
        "print(f\"\\nüî¨ Detailed Token Analysis:\")\n",
        "print(f\"{'Token':<12} {'POS':<8} {'Lemma':<12} {'Is Alpha':<8} {'Is Stop':<8}\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.pos_:<8} {token.lemma_:<12} {token.is_alpha:<8} {token.is_stop:<8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f35eb538"
      },
      "source": [
        "### ü§î Conceptual Question 5\n",
        "**Compare the NLTK and spaCy tokenization results. What differences do you notice? Which approach do you think would be better for different NLP tasks? Consider specific examples like sentiment analysis vs. information extraction.**\n",
        "\n",
        "**Key differences observed:** Both NLTK and spaCy split \"It's\" and separate punctuation. However, spaCy provides more detailed information about each token, like its part of speech (POS), lemma (base form), and whether it's a stop word. NLTK just gives you the tokens themselves.\n",
        "\n",
        "**Better for sentiment analysis:** For sentiment analysis, both could work. NLTK's simpler approach is often enough. But spaCy's ability to identify stop words easily might be helpful if you want to quickly remove common words that don't carry much sentiment.\n",
        "\n",
        "**Better for information extraction:** spaCy is generally better for information extraction. Because it provides POS tags and can identify named entities (like names and places) with its models, it's more powerful for pulling specific pieces of information out of text. NLTK would require more manual steps to get this level of detail.\n",
        "\n",
        "**Overall assessment:** spaCy offers a richer analysis of tokens which is useful for tasks needing deeper linguistic understanding, like information extraction. NLTK is simpler and faster for basic tokenization, suitable for tasks where just splitting words is enough, like some forms of sentiment analysis or simple text counting.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kCVNmEgseV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559a3855-9081-4aea-d45f-8c22eba72766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing on Social Media Text\n",
            "========================================\n",
            "Original: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\n",
            "\n",
            "NLTK tokens: ['OMG', '!', 'Just', 'tried', 'the', 'new', 'coffee', 'shop', '‚òïÔ∏è', 'SO', 'GOOD', '!', '!', '!', 'Highly', 'recommend', 'üëç', '#', 'coffee', '#', 'yum', 'üòç']\n",
            "spaCy tokens: ['OMG', '!', 'Just', 'tried', 'the', 'new', 'coffee', 'shop', '‚òï', 'Ô∏è', 'SO', 'GOOD', '!', '!', '!', 'Highly', 'recommend', 'üëç', '#', 'coffee', '#', 'yum', 'üòç']\n",
            "\n",
            "üìä Comparison:\n",
            "NLTK token count: 22\n",
            "spaCy token count: 23\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Test Tokenization on Complex Text\n",
        "print(\"üß™ Testing on Social Media Text\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "# NLTK approach\n",
        "social_nltk_tokens = word_tokenize(social_text)\n",
        "print(f\"\\nNLTK tokens: {social_nltk_tokens}\")\n",
        "\n",
        "# spaCy approach\n",
        "social_doc = nlp(social_text)\n",
        "social_spacy_tokens = [token.text for token in social_doc]\n",
        "print(f\"spaCy tokens: {social_spacy_tokens}\")\n",
        "\n",
        "print(f\"\\nüìä Comparison:\")\n",
        "print(f\"NLTK token count: {len(social_nltk_tokens)}\")\n",
        "print(f\"spaCy token count: {len(social_spacy_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5699b4a"
      },
      "source": [
        "### ü§î Conceptual Question 6\n",
        "**Looking at how the libraries handled social media text (emojis, hashtags), which library seems more robust for handling \"messy\" real-world text? What specific advantages do you notice? How might this impact a real-world application like social media sentiment analysis?**\n",
        "\n",
        "**More robust library:** Based on the output, spaCy seems a bit more robust for handling messy text like social media.\n",
        "\n",
        "**Specific advantages:** spaCy handled the emoji with skin tone modifier (üëç) as two tokens (üëç and Ô∏è), which is a more accurate representation. NLTK treated the hashtag symbol and the word separately (# and coffee, # and yum). While spaCy also separated them, its overall approach with more detailed token information could be more useful.\n",
        "\n",
        "**Impact on sentiment analysis:** For social media sentiment analysis, handling emojis and hashtags correctly is crucial because they often carry significant sentiment or context. A library that can better process these elements, like spaCy separating the emoji components, might lead to more accurate sentiment analysis results compared to one that treats them as simple punctuation or single, undifferentiated tokens.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkfnnYyweV9_"
      },
      "source": [
        "## üõë Part 4: Stop Words Removal\n",
        "\n",
        "### What are Stop Words?\n",
        "Stop words are common words that appear frequently in a language but typically don't carry much meaningful information about the content. Examples include \"the\", \"is\", \"at\", \"which\", \"on\", etc.\n",
        "\n",
        "### Why Remove Stop Words?\n",
        "1. **Reduce noise** in the data\n",
        "2. **Improve efficiency** by reducing vocabulary size\n",
        "3. **Focus on content words** that carry semantic meaning\n",
        "\n",
        "### When NOT to Remove Stop Words?\n",
        "- **Sentiment analysis:** \"not good\" vs \"good\" - the \"not\" is crucial!\n",
        "- **Question answering:** \"What is the capital?\" - \"what\" and \"is\" provide context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uWwuapZ0eV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2464852-a802-4055-eda9-c0c7c317e35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä NLTK has 198 English stop words\n",
            "First 20: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n",
            "\n",
            "üìä spaCy has 326 English stop words\n",
            "First 20: [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also']\n",
            "\n",
            "üîç Comparison:\n",
            "Common stop words: 123\n",
            "Only in NLTK: 75 - Examples: ['ain', 'aren', \"aren't\", 'couldn', \"couldn't\"]\n",
            "Only in spaCy: 203 - Examples: [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\"]\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Explore Stop Words Lists\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get NLTK English stop words\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "print(f\"üìä NLTK has {len(nltk_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(nltk_stopwords))[:20]}\")\n",
        "\n",
        "# Get spaCy stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words\n",
        "print(f\"\\nüìä spaCy has {len(spacy_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(spacy_stopwords))[:20]}\")\n",
        "\n",
        "# Compare the lists\n",
        "common_stopwords = nltk_stopwords.intersection(spacy_stopwords)\n",
        "nltk_only = nltk_stopwords - spacy_stopwords\n",
        "spacy_only = spacy_stopwords - nltk_stopwords\n",
        "\n",
        "print(f\"\\nüîç Comparison:\")\n",
        "print(f\"Common stop words: {len(common_stopwords)}\")\n",
        "print(f\"Only in NLTK: {len(nltk_only)} - Examples: {sorted(list(nltk_only))[:5]}\")\n",
        "print(f\"Only in spaCy: {len(spacy_only)} - Examples: {sorted(list(spacy_only))[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6469f01a"
      },
      "source": [
        "### ü§î Conceptual Question 7\n",
        "**Why do you think NLTK and spaCy have different stop word lists? Look at the examples of words that are only in one list - do you agree with these choices? Can you think of scenarios where these differences might significantly impact your NLP results?**\n",
        "\n",
        "\n",
        "**Reasons for differences:** Different libraries are built with different goals and might use different criteria to decide which words are \"stop words.\" Some might be more aggressive in removing words, while others might be more conservative.\n",
        "\n",
        "**Agreement with choices:** Looking at the examples, some words only in NLTK's list seem like they *could* be stop words (like \"ain't\"), but others only in spaCy's list (like \"'s\") are also often treated as noise. Whether I agree really depends on the specific task.\n",
        "\n",
        "**Scenarios where differences matter:** These differences can impact results when those specific words are important for the task. For example, if you're analyzing contractions, removing \"'s\" (as spaCy might) could be problematic. If a word only in one list is crucial for sentiment (\"not\" is often *not* a stop word for sentiment), removing it would hurt accuracy.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GrGWsVMReV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db1a64a-80bf-4fe4-f033-ba8cbcf98338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ NLTK Stop Word Removal\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Original tokens (14): ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "After removing stop words (10): ['Natural', 'Language', 'Processing', 'fascinating', 'field', 'AI', '.', \"'s\", 'amazing', '!']\n",
            "\n",
            "Removed words: ['is', 'a', 'of', 'It']\n",
            "Vocabulary reduction: 28.6%\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Remove Stop Words with NLTK\n",
        "# Test on simple text\n",
        "original_tokens = nltk_tokens  # From earlier tokenization\n",
        "filtered_tokens = [word for word in original_tokens if word.lower() not in nltk_stopwords]\n",
        "\n",
        "print(\"üß™ NLTK Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "print(f\"\\nOriginal tokens ({len(original_tokens)}): {original_tokens}\")\n",
        "print(f\"After removing stop words ({len(filtered_tokens)}): {filtered_tokens}\")\n",
        "\n",
        "# Show which words were removed\n",
        "removed_words = [word for word in original_tokens if word.lower() in nltk_stopwords]\n",
        "print(f\"\\nRemoved words: {removed_words}\")\n",
        "\n",
        "# Calculate reduction percentage\n",
        "reduction = (len(original_tokens) - len(filtered_tokens)) / len(original_tokens) * 100\n",
        "print(f\"Vocabulary reduction: {reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lVuS0D-GeV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1591feb2-59f1-4505-c28f-b0f0ca622688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ spaCy Stop Word Removal\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Original tokens (14): ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "After removing stop words & punctuation (7): ['Natural', 'Language', 'Processing', 'fascinating', 'field', 'AI', 'amazing']\n",
            "\n",
            "Removed words: ['is', 'a', 'of', '.', 'It', \"'s\", '!']\n",
            "Vocabulary reduction: 50.0%\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Remove Stop Words with spaCy\n",
        "doc = nlp(simple_text)\n",
        "spacy_filtered = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(\"üß™ spaCy Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "print(f\"\\nOriginal tokens ({len(spacy_tokens)}): {spacy_tokens}\")\n",
        "print(f\"After removing stop words & punctuation ({len(spacy_filtered)}): {spacy_filtered}\")\n",
        "\n",
        "# Show which words were removed\n",
        "spacy_removed = [token.text for token in doc if token.is_stop or token.is_punct]\n",
        "print(f\"\\nRemoved words: {spacy_removed}\")\n",
        "\n",
        "# Calculate reduction percentage\n",
        "spacy_reduction = (len(spacy_tokens) - len(spacy_filtered)) / len(spacy_tokens) * 100\n",
        "print(f\"Vocabulary reduction: {spacy_reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87d9b33a"
      },
      "source": [
        "### ü§î Conceptual Question 8\n",
        "**Compare the NLTK and spaCy stop word removal results. Which approach removed more words? Do you think removing punctuation (as spaCy did) is always a good idea? Give a specific example where keeping punctuation might be important for NLP analysis.**\n",
        "\n",
        "\n",
        "**Which removed more:** spaCy removed more words in this specific example because it removed both stop words and punctuation, whereas NLTK only removed stop words.\n",
        "\n",
        "**Punctuation removal assessment:** Removing punctuation is not always a good idea. While it can help simplify text and reduce noise for some tasks, punctuation can carry important meaning in other contexts.\n",
        "\n",
        "**Example where punctuation matters:** In sentiment analysis, punctuation can significantly impact the perceived emotion. For example, \"This is great!\" expresses stronger positive sentiment than \"This is great.\" Removing the exclamation mark would lose this intensity. Similarly, emoticons like \":)\" or \":(\" are made of punctuation and are crucial for understanding sentiment in informal text.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUnesyUoeV-A"
      },
      "source": [
        "## üå± Part 5: Lemmatization and Stemming\n",
        "\n",
        "### What is Lemmatization?\n",
        "Lemmatization reduces words to their base or dictionary form (called a **lemma**). It considers context and part of speech to ensure the result is a valid word.\n",
        "\n",
        "### What is Stemming?\n",
        "Stemming reduces words to their root form by removing suffixes. It's faster but less accurate than lemmatization.\n",
        "\n",
        "### Key Differences:\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|--------|----------|---------------|\n",
        "| Speed | Fast | Slower |\n",
        "| Accuracy | Lower | Higher |\n",
        "| Output | May be non-words | Always valid words |\n",
        "| Context | Ignores context | Considers context |\n",
        "\n",
        "### Examples:\n",
        "- **\"running\"** ‚Üí Stem: \"run\", Lemma: \"run\"\n",
        "- **\"better\"** ‚Üí Stem: \"better\", Lemma: \"good\"\n",
        "- **\"was\"** ‚Üí Stem: \"wa\", Lemma: \"be\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RhQlU7wGeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62db0174-34ee-4dc4-d296-75bfeab47f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåø Stemming Demonstration\n",
            "==============================\n",
            "Original     Stemmed     \n",
            "-------------------------\n",
            "running      run         \n",
            "runs         run         \n",
            "ran          ran         \n",
            "better       better      \n",
            "good         good        \n",
            "best         best        \n",
            "flying       fli         \n",
            "flies        fli         \n",
            "was          wa          \n",
            "were         were        \n",
            "cats         cat         \n",
            "dogs         dog         \n",
            "\n",
            "üß™ Applied to sample text:\n",
            "Original: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', 'It', 'amazing']\n",
            "Stemmed: ['natur', 'languag', 'process', 'is', 'a', 'fascin', 'field', 'of', 'ai', 'it', 'amaz']\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Stemming with NLTK\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Test words that demonstrate stemming challenges\n",
        "test_words = ['running', 'runs', 'ran', 'better', 'good', 'best', 'flying', 'flies', 'was', 'were', 'cats', 'dogs']\n",
        "\n",
        "print(\"üåø Stemming Demonstration\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12}\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "for word in test_words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    print(f\"{word:<12} {stemmed:<12}\")\n",
        "\n",
        "# Apply to our sample text\n",
        "sample_tokens = [token for token in nltk_tokens if token.isalpha()]\n",
        "stemmed_tokens = [stemmer.stem(token.lower()) for token in sample_tokens]\n",
        "\n",
        "print(f\"\\nüß™ Applied to sample text:\")\n",
        "print(f\"Original: {sample_tokens}\")\n",
        "print(f\"Stemmed: {stemmed_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "498096cf"
      },
      "source": [
        "### ü§î Conceptual Question 9\n",
        "**Look at the stemming results above. Can you identify any cases where stemming produced questionable results? For example, how were \"better\" and \"good\" handled? Do you think this is problematic for NLP applications? Explain your reasoning.**\n",
        "\n",
        "\n",
        "**Questionable results identified:** Stemming produced questionable results for words like \"ran\" (stemmed to \"ran\"), \"better\" and \"good\" (stemmed to themselves), \"flying\" and \"flies\" (both stemmed to \"fli\"), and \"was\" (stemmed to \"wa\"). These stemmed forms are not always actual words.\n",
        "\n",
        "**Assessment of \"better\" and \"good\":** \"Better\" and \"good\" are related in meaning but stemming didn't reduce them to a common root. They were left as they were, which means a system using stemming wouldn't recognize them as variations of the same concept.\n",
        "\n",
        "**Impact on NLP applications:** This can be problematic. For a search engine, if someone searches for \"running shoes,\" stemming \"running\" to \"run\" is fine. But if they search for \"best price,\" and \"best\" isn't stemmed to \"good,\" the search might miss results containing \"good price.\" In sentiment analysis, stemming \"better\" and \"good\" separately could prevent the system from recognizing that both indicate positive sentiment. Stemming's aggressive approach can sometimes group unrelated words or fail to group related ones, impacting the accuracy of tasks that rely on understanding word relationships.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VYhyO8jfeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a200c21-7890-4493-9824-4e64123b515c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå± spaCy Lemmatization Demonstration\n",
            "========================================\n",
            "Original: The researchers were studying the effects of running and swimming on better performance.\n",
            "\n",
            "Token           Lemma           POS        Explanation         \n",
            "-----------------------------------------------------------------\n",
            "The             the             DET        No change           \n",
            "researchers     researcher      NOUN       Lemmatized          \n",
            "were            be              AUX        Lemmatized          \n",
            "studying        study           VERB       Lemmatized          \n",
            "the             the             DET        No change           \n",
            "effects         effect          NOUN       Lemmatized          \n",
            "of              of              ADP        No change           \n",
            "running         run             VERB       Lemmatized          \n",
            "and             and             CCONJ      No change           \n",
            "swimming        swim            VERB       Lemmatized          \n",
            "on              on              ADP        No change           \n",
            "better          well            ADJ        Lemmatized          \n",
            "performance     performance     NOUN       No change           \n",
            "\n",
            "üî§ Lemmatized tokens (no stop words): ['researcher', 'study', 'effect', 'run', 'swim', 'well', 'performance']\n"
          ]
        }
      ],
      "source": [
        "# Step 11: Lemmatization with spaCy\n",
        "print(\"üå± spaCy Lemmatization Demonstration\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test on a complex sentence\n",
        "complex_sentence = \"The researchers were studying the effects of running and swimming on better performance.\"\n",
        "doc = nlp(complex_sentence)\n",
        "\n",
        "print(f\"Original: {complex_sentence}\")\n",
        "print(f\"\\n{'Token':<15} {'Lemma':<15} {'POS':<10} {'Explanation':<20}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_alpha:\n",
        "        explanation = \"No change\" if token.text.lower() == token.lemma_ else \"Lemmatized\"\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {explanation:<20}\")\n",
        "\n",
        "# Extract lemmas\n",
        "lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "print(f\"\\nüî§ Lemmatized tokens (no stop words): {lemmas}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tsCdhYHWeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ece31f6-fd37-4f25-a1a5-8ba73d842db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è Stemming vs Lemmatization Comparison\n",
            "==================================================\n",
            "Original     Stemmed      Lemmatized  \n",
            "----------------------------------------\n",
            "better       better       well        \n",
            "running      run          run         \n",
            "studies      studi        study       \n",
            "was          wa           be          \n",
            "children     children     child       \n",
            "feet         feet         foot        \n"
          ]
        }
      ],
      "source": [
        "# Step 12: Compare Stemming vs Lemmatization\n",
        "comparison_words = ['better', 'running', 'studies', 'was', 'children', 'feet']\n",
        "\n",
        "print(\"‚öñÔ∏è Stemming vs Lemmatization Comparison\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12} {'Lemmatized':<12}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for word in comparison_words:\n",
        "    # Stemming\n",
        "    stemmed = stemmer.stem(word)\n",
        "\n",
        "    # Lemmatization with spaCy\n",
        "    doc = nlp(word)\n",
        "    lemmatized = doc[0].lemma_\n",
        "\n",
        "    print(f\"{word:<12} {stemmed:<12} {lemmatized:<12}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8071f8c2"
      },
      "source": [
        "### ü§î Conceptual Question 10\n",
        "**Compare the stemming and lemmatization results. Which approach do you think is more suitable for:**\n",
        "1. **A search engine** (where speed is crucial and you need to match variations of words)?\n",
        "2. **A sentiment analysis system** (where accuracy and meaning preservation are important)?\n",
        "3. **A real-time chatbot** (where both speed and accuracy matter)?\n",
        "\n",
        "**Explain your reasoning for each choice.**\n",
        "\n",
        "**1. Search engine:** Stemming is often better for a search engine. Speed is very important when someone searches, and stemming is faster. It's good enough to find variations of words even if the root isn't a real word. For example, stemming \"running,\" \"runs,\" and \"ran\" to \"run\" helps the search find documents with any of those words when the user searches for \"run.\"\n",
        "\n",
        "**2. Sentiment analysis:** Lemmatization is usually better for sentiment analysis. Accuracy and keeping the word's real meaning are key. Lemmatization gives you a valid base word (\"better\" to \"good\") which helps the system understand the true sentiment. Stemming might create non-words or fail to group related words, which could confuse the sentiment analysis.\n",
        "\n",
        "**3. Real-time chatbot:** This is tricky and might depend on the chatbot's specific job. If speed is the absolute top priority, stemming might be chosen. However, for better understanding of user input and more accurate responses, especially in complex conversations, lemmatization is likely preferred despite being a bit slower. A hybrid approach or using pre-computed lemmas could also be an option to balance speed and accuracy.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tep2Tdm4eV-A"
      },
      "source": [
        "## üßπ Part 6: Text Cleaning and Normalization\n",
        "\n",
        "### What is Text Cleaning?\n",
        "Text cleaning involves removing or standardizing elements that might interfere with analysis:\n",
        "- **Case normalization** (converting to lowercase)\n",
        "- **Punctuation removal**\n",
        "- **Number handling** (remove, replace, or normalize)\n",
        "- **Special character handling** (URLs, emails, mentions)\n",
        "- **Whitespace normalization**\n",
        "\n",
        "### Why is it Important?\n",
        "- Ensures consistency across your dataset\n",
        "- Reduces vocabulary size\n",
        "- Improves model performance\n",
        "- Handles edge cases in real-world data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lmo2dAXkeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87578f69-d97d-414a-802a-315c65c5e5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Basic Text Cleaning\n",
            "==============================\n",
            "Original: '   Hello WORLD!!! This has 123 numbers and   extra spaces.   '\n",
            "Cleaned: 'hello world this has numbers and extra spaces'\n",
            "Length reduction: 26.2%\n"
          ]
        }
      ],
      "source": [
        "# Step 13: Basic Text Cleaning\n",
        "def basic_clean_text(text):\n",
        "    \"\"\"Apply basic text cleaning operations\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces again\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test basic cleaning\n",
        "test_text = \"   Hello WORLD!!! This has 123 numbers and   extra spaces.   \"\n",
        "cleaned = basic_clean_text(test_text)\n",
        "\n",
        "print(\"üßπ Basic Text Cleaning\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Original: '{test_text}'\")\n",
        "print(f\"Cleaned: '{cleaned}'\")\n",
        "print(f\"Length reduction: {(len(test_text) - len(cleaned))/len(test_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FxHKo_2ceV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd790de-8191-44d4-f306-7efd8db3dabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Advanced Cleaning on Social Media Text\n",
            "=============================================\n",
            "Original: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\n",
            "Cleaned: omg! just tried the new coffee shop ‚òïÔ∏è so good!!! highly recommend coffee yum\n",
            "Length reduction: 7.2%\n"
          ]
        }
      ],
      "source": [
        "# Step 14: Advanced Cleaning for Social Media\n",
        "def advanced_clean_text(text):\n",
        "    \"\"\"Apply advanced cleaning for social media and web text\"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Convert hashtags (keep the word, remove #)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    # Remove emojis (basic approach)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Convert to lowercase and normalize whitespace\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test on social media text\n",
        "print(\"üöÄ Advanced Cleaning on Social Media Text\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "cleaned_social = advanced_clean_text(social_text)\n",
        "print(f\"Cleaned: {cleaned_social}\")\n",
        "print(f\"Length reduction: {(len(social_text) - len(cleaned_social))/len(social_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88536934"
      },
      "source": [
        "### ü§î Conceptual Question 11\n",
        "**Look at the advanced cleaning results for the social media text. What information was lost during cleaning? Can you think of scenarios where removing emojis and hashtags might actually hurt your NLP application? What about scenarios where keeping them would be beneficial?**\n",
        "\n",
        "\n",
        "**Information lost:** During advanced cleaning, information like URLs, email addresses, mentions (@usernames), and the original hashtag symbols (#) were removed. Some specific emoji variations might also have been simplified or removed depending on the exact pattern used.\n",
        "\n",
        "**Scenarios where removal hurts:** Removing emojis and hashtags can hurt NLP applications where they carry significant meaning or sentiment. For example, in sentiment analysis of social media, an emoji like üòÇ or a hashtag like #blessed strongly indicate positive sentiment. Removing them would strip away this crucial emotional information. Similarly, for tasks like identifying trending topics or understanding social network interactions, removing hashtags and mentions would eliminate valuable data.\n",
        "\n",
        "**Scenarios where keeping helps:** Keeping emojis and hashtags is beneficial when they provide important context, sentiment, or structural information. For sentiment analysis, as mentioned, emojis are key indicators. For topic modeling or trend analysis, hashtags define themes. For social network analysis, mentions show interactions between users. In these cases, preserving these elements allows the NLP system to capture a more complete understanding of the text and its social context.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpzKWVy3eV-A"
      },
      "source": [
        "## üîß Part 7: Building a Complete Preprocessing Pipeline\n",
        "\n",
        "Now let's combine everything into a comprehensive preprocessing pipeline that you can customize based on your needs.\n",
        "\n",
        "### Pipeline Components:\n",
        "1. **Text cleaning** (basic or advanced)\n",
        "2. **Tokenization** (NLTK or spaCy)\n",
        "3. **Stop word removal** (optional)\n",
        "4. **Lemmatization/Stemming** (optional)\n",
        "5. **Additional filtering** (length, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6XxqZCHneV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7582d9d7-e949-455c-c57a-5f5849f3c09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Preprocessing Pipeline Created!\n",
            "‚úÖ Ready to test different configurations.\n"
          ]
        }
      ],
      "source": [
        "# Step 15: Complete Preprocessing Pipeline\n",
        "def preprocess_text(text,\n",
        "                   clean_level='basic',     # 'basic' or 'advanced'\n",
        "                   remove_stopwords=True,\n",
        "                   use_lemmatization=True,\n",
        "                   use_stemming=False,\n",
        "                   min_length=2):\n",
        "    \"\"\"\n",
        "    Complete text preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Step 1: Clean text\n",
        "    if clean_level == 'basic':\n",
        "        cleaned_text = basic_clean_text(text)\n",
        "    else:\n",
        "        cleaned_text = advanced_clean_text(text)\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    if use_lemmatization:\n",
        "        # Use spaCy for lemmatization\n",
        "        doc = nlp(cleaned_text)\n",
        "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
        "    else:\n",
        "        # Use NLTK for basic tokenization\n",
        "        tokens = word_tokenize(cleaned_text)\n",
        "        tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Step 3: Remove stop words\n",
        "    if remove_stopwords:\n",
        "        if use_lemmatization:\n",
        "            tokens = [token for token in tokens if token not in spacy_stopwords]\n",
        "        else:\n",
        "            tokens = [token.lower() for token in tokens if token.lower() not in nltk_stopwords]\n",
        "\n",
        "    # Step 4: Apply stemming if requested\n",
        "    if use_stemming and not use_lemmatization:\n",
        "        tokens = [stemmer.stem(token.lower()) for token in tokens]\n",
        "\n",
        "    # Step 5: Filter by length\n",
        "    tokens = [token for token in tokens if len(token) >= min_length]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "print(\"üîß Preprocessing Pipeline Created!\")\n",
        "print(\"‚úÖ Ready to test different configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yYMlNDVceV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98f1001-bf18-41cd-cd4f-ae3280ef6003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Testing on: This laptop is absolutely fantastic! I've been using it for 6 months and it's still super fast.\n",
            "The ...\n",
            "============================================================\n",
            "\n",
            "1. Minimal processing (34 tokens):\n",
            "   ['this', 'laptop', 'is', 'absolutely', 'fantastic', 'ive', 'been', 'using', 'it', 'for']...\n",
            "\n",
            "2. Standard processing (18 tokens):\n",
            "   ['laptop', 'absolutely', 'fantastic', 've', 'use', 'month', 'super', 'fast', 'battery', 'life']...\n",
            "\n",
            "3. Aggressive processing (21 tokens):\n",
            "   ['laptop', 'absolut', 'fantast', 'use', 'month', 'still', 'super', 'fast', 'batteri', 'life']...\n",
            "\n",
            "üìä Token Reduction Summary:\n",
            "   Original: 47 tokens\n",
            "   Minimal: 34 (27.7% reduction)\n",
            "   Standard: 18 (61.7% reduction)\n",
            "   Aggressive: 21 (55.3% reduction)\n"
          ]
        }
      ],
      "source": [
        "# Step 16: Test Different Pipeline Configurations\n",
        "test_text = sample_texts[\"Product Review\"]\n",
        "print(f\"üéØ Testing on: {test_text[:100]}...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuration 1: Minimal processing\n",
        "minimal = preprocess_text(test_text,\n",
        "                         clean_level='basic',\n",
        "                         remove_stopwords=False,\n",
        "                         use_lemmatization=False,\n",
        "                         use_stemming=False)\n",
        "print(f\"\\n1. Minimal processing ({len(minimal)} tokens):\")\n",
        "print(f\"   {minimal[:10]}...\")\n",
        "\n",
        "# Configuration 2: Standard processing\n",
        "standard = preprocess_text(test_text,\n",
        "                          clean_level='basic',\n",
        "                          remove_stopwords=True,\n",
        "                          use_lemmatization=True)\n",
        "print(f\"\\n2. Standard processing ({len(standard)} tokens):\")\n",
        "print(f\"   {standard[:10]}...\")\n",
        "\n",
        "# Configuration 3: Aggressive processing\n",
        "aggressive = preprocess_text(test_text,\n",
        "                            clean_level='advanced',\n",
        "                            remove_stopwords=True,\n",
        "                            use_lemmatization=False,\n",
        "                            use_stemming=True,\n",
        "                            min_length=3)\n",
        "print(f\"\\n3. Aggressive processing ({len(aggressive)} tokens):\")\n",
        "print(f\"   {aggressive[:10]}...\")\n",
        "\n",
        "# Show reduction percentages\n",
        "original_count = len(word_tokenize(test_text))\n",
        "print(f\"\\nüìä Token Reduction Summary:\")\n",
        "print(f\"   Original: {original_count} tokens\")\n",
        "print(f\"   Minimal: {len(minimal)} ({(original_count-len(minimal))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Standard: {len(standard)} ({(original_count-len(standard))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Aggressive: {len(aggressive)} ({(original_count-len(aggressive))/original_count*100:.1f}% reduction)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e02104"
      },
      "source": [
        "### ü§î Conceptual Question 12\n",
        "**Compare the three pipeline configurations (Minimal, Standard, Aggressive). For each configuration, analyze:**\n",
        "1. **What information was preserved?**\n",
        "2. **What information was lost?**\n",
        "3. **What type of NLP task would this configuration be best suited for?**\n",
        "\n",
        "**Minimal Processing:**\n",
        "- Preserved: Most of the original words and punctuation are kept. Case is normalized.\n",
        "- Lost: Only extra whitespace and basic capitalization differences are removed.\n",
        "- Best for: Tasks where the exact wording and structure are important, like grammar checking, part-of-speech tagging, or when you need a large vocabulary.\n",
        "\n",
        "**Standard Processing:**\n",
        "- Preserved: Core meaningful words are kept. Variations of words are reduced to their base form (lemmatization).\n",
        "- Lost: Punctuation, numbers, and common stop words are removed. Some subtle meaning from word variations might be lost.\n",
        "- Best for: Many common NLP tasks like text classification, topic modeling, or information retrieval where focusing on the main content words is beneficial.\n",
        "\n",
        "**Aggressive Processing:**\n",
        "- Preserved: Only the most basic word roots are kept. More aggressive cleaning removes URLs, mentions, and emojis.\n",
        "- Lost: Significant structural and contextual information is lost (punctuation, stop words, numbers, special characters). Word roots may not be actual words.\n",
        "- Best for: Tasks where reducing dimensionality and focusing on very core word components is critical, often in highly specific or experimental scenarios, or for tasks where speed is paramount and some loss of nuance is acceptable (though lemmatization is usually preferred over stemming for accuracy).\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rtgwLN7neV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c362aef-5877-445a-8a0e-30eedda46943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Comprehensive Preprocessing Analysis\n",
            "==================================================\n",
            "\n",
            "üìÑ Simple:\n",
            "   Original: 14 tokens\n",
            "   Processed: 7 tokens (50.0% reduction)\n",
            "   Sample: ['natural', 'language', 'processing', 'fascinating', 'field', 'ai', 'amazing']\n",
            "\n",
            "üìÑ Academic:\n",
            "   Original: 61 tokens\n",
            "   Processed: 26 tokens (57.4% reduction)\n",
            "   Sample: ['dr', 'smith', 'research', 'machinelearning', 'algorithm', 'groundbreake', 'publish', 'paper']\n",
            "\n",
            "üìÑ Social Media:\n",
            "   Original: 22 tokens\n",
            "   Processed: 10 tokens (54.5% reduction)\n",
            "   Sample: ['omg', 'try', 'new', 'coffee', 'shop', 'good', 'highly', 'recommend']\n",
            "\n",
            "üìÑ News:\n",
            "   Original: 51 tokens\n",
            "   Processed: 25 tokens (51.0% reduction)\n",
            "   Sample: ['stock', 'market', 'experience', 'significant', 'volatility', 'today', 'tech', 'stock']\n",
            "\n",
            "üìÑ Product Review:\n",
            "   Original: 47 tokens\n",
            "   Processed: 18 tokens (61.7% reduction)\n",
            "   Sample: ['laptop', 'absolutely', 'fantastic', 've', 'use', 'month', 'super', 'fast']\n",
            "\n",
            "\n",
            "üìã Summary Table\n",
            "Text Type       Original   Processed  Reduction \n",
            "--------------------------------------------------\n",
            "Simple          14         7          50.0      %\n",
            "Academic        61         26         57.4      %\n",
            "Social Media    22         10         54.5      %\n",
            "News            51         25         51.0      %\n",
            "Product Review  47         18         61.7      %\n"
          ]
        }
      ],
      "source": [
        "# Step 17: Comprehensive Analysis Across Text Types\n",
        "print(\"üî¨ Comprehensive Preprocessing Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test standard preprocessing on all text types\n",
        "results = {}\n",
        "for name, text in sample_texts.items():\n",
        "    original_tokens = len(word_tokenize(text))\n",
        "    processed_tokens = preprocess_text(text,\n",
        "                                      clean_level='basic',\n",
        "                                      remove_stopwords=True,\n",
        "                                      use_lemmatization=True)\n",
        "\n",
        "    reduction = (original_tokens - len(processed_tokens)) / original_tokens * 100\n",
        "    results[name] = {\n",
        "        'original': original_tokens,\n",
        "        'processed': len(processed_tokens),\n",
        "        'reduction': reduction,\n",
        "        'sample': processed_tokens[:8]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìÑ {name}:\")\n",
        "    print(f\"   Original: {original_tokens} tokens\")\n",
        "    print(f\"   Processed: {len(processed_tokens)} tokens ({reduction:.1f}% reduction)\")\n",
        "    print(f\"   Sample: {processed_tokens[:8]}\")\n",
        "\n",
        "# Summary table\n",
        "print(f\"\\n\\nüìã Summary Table\")\n",
        "print(f\"{'Text Type':<15} {'Original':<10} {'Processed':<10} {'Reduction':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for name, data in results.items():\n",
        "    print(f\"{name:<15} {data['original']:<10} {data['processed']:<10} {data['reduction']:<10.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b34eac"
      },
      "source": [
        "### ü§î Final Conceptual Question 13\n",
        "**Looking at the comprehensive analysis results across all text types:**\n",
        "\n",
        "1. **Which text type was most affected by preprocessing?** Why do you think this happened?\n",
        "2. **Which text type was least affected?** What does this tell you about the nature of that text?\n",
        "3. **If you were building an NLP system to analyze customer reviews for a business, which preprocessing approach would you choose and why?**\n",
        "4. **What are the main trade-offs you need to consider when choosing preprocessing techniques for any NLP project?**\n",
        "\n",
        "**1. Most affected text type:** Product Review was the most affected, showing the highest percentage reduction in tokens (61.7%). This likely happened because product reviews often contain more informal language, numbers (ratings), and potentially more stop words compared to more formal text types. The standard preprocessing pipeline (basic cleaning, stop word removal, lemmatization) would aggressively target these elements, leading to a higher reduction.\n",
        "\n",
        "**2. Least affected text type:** Simple text was the least affected, with a 50.0% reduction. This tells us that simple text is already relatively clean and contains fewer complex structures, special characters, or a high density of stop words compared to the other text types. It's closer to the \"ideal\" input for standard NLP processing, so less is removed.\n",
        "\n",
        "**3. For customer review analysis:** I would likely choose a **Standard Processing** approach, possibly with some modifications. Customer reviews contain sentiment, which means removing *all* stop words might be harmful (\"not good\"). I would likely keep negation words. Lemmatization is important to group word variations (\"fantastic,\" \"fantastically\") for consistent sentiment analysis. Basic cleaning is good for removing punctuation and numbers (though extracting the rating itself would need a specific step before this). Aggressive cleaning might remove emojis or informal language crucial for sentiment.\n",
        "\n",
        "**4. Main trade-offs to consider:** When choosing preprocessing techniques for any NLP project, you need to consider several trade-offs. One is the balance between **information loss and noise reduction**. More aggressive techniques reduce noise but can also remove valuable data needed for certain tasks, such as sentiment conveyed by emojis. Another trade-off is **speed versus accuracy**. Stemming is faster but less accurate than lemmatization; the choice depends on whether real-time performance or precise meaning is more critical for your application. You also need to think about **complexity versus robustness**. More complex cleaning methods handle messy text better but might be unnecessary for cleaner text and require more specific rules. Finally, there's the consideration of **generalization versus task specificity**. A general pipeline can be used for many tasks, but customizing it for your exact project will usually lead to better performance.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byb97qokeV-G"
      },
      "source": [
        "## üéØ Lab Summary and Reflection\n",
        "\n",
        "Congratulations! You've completed a comprehensive exploration of NLP preprocessing techniques.\n",
        "\n",
        "### üîë Key Concepts You've Mastered:\n",
        "\n",
        "1. **Text Preprocessing Fundamentals** - Understanding why preprocessing is crucial\n",
        "2. **Tokenization Techniques** - NLTK vs spaCy approaches and their trade-offs\n",
        "3. **Stop Word Management** - When to remove them and when to keep them\n",
        "4. **Morphological Processing** - Stemming vs lemmatization for different use cases\n",
        "5. **Text Cleaning Strategies** - Basic vs advanced cleaning for different text types\n",
        "6. **Pipeline Design** - Building modular, configurable preprocessing systems\n",
        "\n",
        "### üéì Real-World Applications:\n",
        "These techniques form the foundation for search engines, chatbots, sentiment analysis, document classification, machine translation, and information extraction systems.\n",
        "\n",
        "### üí° Key Insights to Remember:\n",
        "- **No Universal Solution**: Different NLP tasks require different preprocessing approaches\n",
        "- **Trade-offs Are Everywhere**: Balance information preservation with noise reduction\n",
        "- **Context Matters**: The same technique can help or hurt depending on your use case\n",
        "- **Experimentation Is Key**: Always test and measure impact on your specific task\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work completing Lab 02!** üéâ\n",
        "\n",
        "For your reflection journal, focus on the insights you gained about when and why to use different techniques, the challenges you encountered, and connections you made to real-world applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}